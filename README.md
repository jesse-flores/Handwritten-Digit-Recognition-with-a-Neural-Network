# Handwritten Digit Recognition with a Neural Network

This project is a C++ application that uses a custom-built neural network to recognize handwritten digits (0-9). It features an interactive GUI built with SFML where you can draw a digit with your mouse and have the network predict what you've drawn. The project also includes a separate script for training the neural network from scratch using the famous MNIST dataset.

## Features

- **Interactive GUI:** Draw digits with your mouse on a canvas.
- **Neural Network Prediction:** Uses a feedforward neural network to predict the drawn digit in real-time.
- **Model Training:** Includes a `train` executable to build and save the neural network model using the MNIST dataset.
- **Flexible:** Built with standard C++ and SFML, making it portable.

## Project Structure

    root/
    │
    ├── sfml/                 # Contains SFML library files (headers, libs)
    │
    ├── gui.cpp               # Source code for the interactive drawing and prediction GUI.
    ├── main.cpp              # Source code for training the neural network model.
    ├── neural_network.cpp    # Implementation of the Neural Network class.
    ├── neural_network.h      # Header file for the Neural Network.
    ├── mnist_reader.cpp      # Helper functions to read the MNIST dataset files.
    ├── mnist_reader.h        # Header file for the MNIST reader.
    ├── makefile              # The build script for compiling the project.
    │
    ├── mnist_model.dat       # The trained model weights (generated by ./train).
    ├── train.exe             # The compiled training program.
    └── gui.exe               # The compiled GUI program.


## Setup & Compilation

This project is configured to be built on **Windows** using a **MinGW-w64 (UCRT) 64-bit** compiler and a **static** version of the SFML 3.0.1 library.

### Prerequisites

1.  **C++ Compiler:** A C++17 compatible compiler. The `makefile` is configured for `g++` from a **MinGW-w64 (UCRT) 64-bit** environment (like the one provided by MSYS2).
2.  **SFML 3.0.0 (or newer):** You must use the correct version of SFML that matches your compiler.
  - Download the version for **GCC MinGW (UCRT) - 64-bit** from the [official SFML download page](https://www.sfml-dev.org/download/sfml/3.0.0/).
3.  **Make:** The `make` utility is required to use the provided `makefile`. It is available in development environments like MSYS2.

### Configuration Steps

1.  **Configure SFML:**
  - this program was adjusted to work with the latest 3.0.1 SFML version
  - Dowload the zip file for the official sfml dowload page: [https://www.sfml-dev.org/download/sfml/3.0.0/](https://www.sfml-dev.org/download/sfml/3.0.0/).
  - From the SFML zip file you downloaded, copy the `include` and `lib` folders into your new `sfml` directory. Your project path should look like `root/sfml/lib/` and `root/sfml/include/`.

2.  **Download MNIST Dataset (for training):**
  - To run the training executable (`./train`), you must first download the MNIST dataset files.
  - In previous years, you'd get the files from [Yann Lecun's website](http://yann.lecun.com/exdb/mnist/) but an alternative is accessing these files through the [Wayback Maching version of it](https://web.archive.org/web/20060206113955/http://yann.lecun.com/exdb/mnist/).
### Compilation

1. **Clone the project repository:**
```bash
git clone https://github.com/jesse-flores/SFML-Digit-Recognizer.git
```

2. **Navigate to the directory:**
```bash
cd Handwritten-Digit-Recognition-with-a-Neural-Network
```

The provided `makefile` handles the entire compilation process for a windows copmuter.

3. **Compile the Training Program:**
  This command creates the `train.exe` executable.
  ```bash
  make train
  ```

4. **Compile the GUI Program:**
  This command creates the `gui.exe` executable.
  ```bash
  make gui
  ```

5. **Clean Up Build Files:**
  This command removes all compiled object files (`.o`) and executables (`.exe`) except the model trained.
  ```bash
  make clean
  ```

## Usage

You must train the model at least once before you can use the GUI for predictions.

1.  **Train the Model:**
  Run the compiled training program. This will read the MNIST dataset, train the neural network, and create a file named `mnist_model.dat` containing the learned weights.
  ```bash
  ./train
  ```

2.  **Run the GUI:**
  After the `mnist_model.dat` file has been created, you can run the GUI program.
  ```bash
  ./gui
  ```

### GUI Controls

- **Draw with Mouse:** Hold the left mouse button and move the mouse to draw a digit on the black canvas.
- **Predict:** Press the **`Space`** key. The console will print the network's prediction and its confidence level.
- **Clear Canvas:** Press the **`C`** key to erase the canvas.
- **Close:** Press the **`Escape`** key or click the window's close button.

## Adjusting the Model

You can fine-tune the neural network's architecture and training process by modifying several key parameters in the `main.cpp` file. After making any changes, you must recompile and retrain the model.

**Workflow:**
1.  Edit the parameters in `main.cpp`.
2.  Run `make train` to recompile.
3.  Run `./train` to generate a new `mnist_model.dat`.

---

### Network Architecture (`layer_sizes`)

This vector defines the structure of the network. The first number is the input size (784 pixels), the last is the output size (10 digits), and the numbers in between are the hidden layers.

-   **To make the network deeper:** Add more numbers in the middle.
-   **To make layers wider:** Increase the value of the numbers.

Deeper/wider networks can learn more complex patterns but take longer to train and are more prone to overfitting.

```cpp
// In main.cpp
vector<int> layer_sizes = {784, 128, 64, 10};
```

### Epochs
An epoch is one complete pass through the entire training dataset. More epochs can lead to better accuracy, but too many can cause the model to "memorize" the training data (overfitting) and perform poorly on new data.

```cpp
// In main.cpp
int epochs = 10;
```

### Learning Rate
This parameter controls how much the network's weights are adjusted during each update.
- Too high: The model might learn quickly but overshoot the optimal solution and become unstable.
- Too low: Training will be very slow and may get stuck in a suboptimal solution.

A common practice is to start with a value like 0.01 or 0.05 and adjust from there.

```cpp
// In main.cpp
double learning_rate = 0.05;
```

### Batch Size
This is the number of training samples the network processes before updating its weights.
- Larger batches (e.g., 64, 128): Lead to more stable and faster training but require more memory.
- Smaller batches (e.g., 16, 32): Updates are more frequent and noisy, which can sometimes help the model find a better solution.

```cpp
// In main.cpp
size_t batch_size = 32;
```

## Example
### Training
    PS C:\handwritting_detection\> ./train
    Loading MNIST dataset...
    Initializing neural network...
    Starting training...
    Epoch 1/10, Loss: 0.322733
    Epoch 2/10, Loss: 0.150471
    Epoch 3/10, Loss: 0.108773
    Epoch 4/10, Loss: 0.084474
    Epoch 5/10, Loss: 0.0689049
    Epoch 6/10, Loss: 0.057203
    Epoch 7/10, Loss: 0.0484788
    Epoch 8/10, Loss: 0.0402465
    Epoch 9/10, Loss: 0.034386
    Epoch 10/10, Loss: 0.0284253
    Training complete.
    Evaluating network on test data...
    ----------------------------------------
    Overall Test Accuracy: 97.53%
    Correctly predicted 9753 out of 10000 test images.
    ----------------------------------------
    Confidence Threshold: 90%
    Number of 'sure' predictions: 9506 (95.06%)
    Accuracy on 'sure' predictions: 99.2531%
    ----------------------------------------
    Training complete.
    Model Saved!
    PS C:\handwritting_detection\>
### GUI

Visual to Come Soon


## Dependencies

- C++17
- SFML >= 3.0.0 (Static Build)

## License


This project is open source and available under the [MIT License](LICENSE).